{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJDm5_NK4my"
      },
      "source": [
        "# HW 2 (CS492 Machine Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23kAFAHslQz"
      },
      "outputs": [],
      "source": [
        "# Part 1: Implement a function get_primes() that finds all prime numbers between\n",
        "# a lower bound and a upper bound (inclusive).\n",
        "# For example, if lower = 1, upper = 10, it returns [2, 3, 5, 7].\n",
        "\n",
        "# TODO: implement get_primes() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JnSF-5kLQrM",
        "outputId": "3eb4afad-3360-4a71-b8b4-91d8bd6f5d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
            "[2]\n",
            "[]\n",
            "[2]\n",
            "[2, 3, 5]\n"
          ]
        }
      ],
      "source": [
        "def get_primes(lower, upper):\n",
        "    if lower > upper:\n",
        "        return []\n",
        "\n",
        "    primes = []\n",
        "    for num in range(max(2, lower), upper + 1):  # Ensure we start at least from 2\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):  # Check divisibility up to sqrt(num)\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "\n",
        "    return primes\n",
        "\n",
        "# Test cases\n",
        "print(get_primes(1, 100))  # Expected output: List of primes from 1 to 100\n",
        "print(get_primes(1, 2))    # Expected output: [2]\n",
        "print(get_primes(2, 1))    # Expected output: []\n",
        "print(get_primes(2, 2))    # Expected output: [2]\n",
        "print(get_primes(-20, 5))  # Expected output: [2, 3, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3TBkVtiNHlN"
      },
      "outputs": [],
      "source": [
        "# Part 2: Implement a function find_item_sorted_array() that finds\n",
        "# if an given item is in a sorted array or not, by using binary search.\n",
        "# For example, if array = [1, 2, 3] and item = 2, it returns 1.\n",
        "# If array = [1, 2, 3] and item = 4, it returns -1.\n",
        "\n",
        "# TODO: implement find_item_sorted_array() function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xinoI80qNwNH"
      },
      "outputs": [],
      "source": [
        "def get_primes(lower, upper):\n",
        "    if lower > upper:\n",
        "        return []\n",
        "\n",
        "    primes = []\n",
        "    for num in range(max(2, lower), upper + 1):  # Ensure we start at least from 2\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):  # Check divisibility up to sqrt(num)\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "\n",
        "    return primes\n",
        "\n",
        "def find_item_sorted_array(array, item):\n",
        "    left, right = 0, len(array) - 1\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        if array[mid] == item:\n",
        "            return mid\n",
        "        elif array[mid] < item:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return -1\n",
        "\n",
        "# Test cases\n",
        "print(get_primes(1, 100))  # Expected output: List of primes from 1 to 100\n",
        "print(get_primes(1, 2))    # Expected output: [2]\n",
        "print(get_primes(2, 1))    # Expected output: []\n",
        "print(get_primes(2, 2))    # Expected output: [2]\n",
        "print(get_primes(-20, 5))  # Expected output: [2, 3, 5]\n",
        "\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 0))  # Expected output: -1\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 1))  # Expected output: 0\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 3))  # Expected output: 2\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 5))  # Expected output: 4\n",
        "print(find_item_sorted_array([1, 2, 3, 4, 5], 6))  # Expected output: -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWD0tXACRlZj"
      },
      "outputs": [],
      "source": [
        "# Part 3: ChatBot.\n",
        "\n",
        "# TODO: obtain OPENAI API Key\n",
        "\n",
        "# TODO: install missing modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YCdwfelQmNL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "ad377505-dc04-4c7c-f466-0c13763817c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5def4f88d5fd14b150.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5def4f88d5fd14b150.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from typing import List, Tuple\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Function for Chatbot Response\n",
        "def chatbot_response(user_input, history):\n",
        "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    # Convert history into the correct format (list of tuples)\n",
        "    formatted_history = [(message[0], message[1]) for message in history]\n",
        "\n",
        "    # Add formatted history to the messages list\n",
        "    for message in formatted_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": message[0]})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": message[1]})\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=1.0,\n",
        "        max_tokens=200\n",
        "    )\n",
        "\n",
        "    assistant_reply = response.choices[0].message.content\n",
        "\n",
        "    # Append the user input and assistant reply as a tuple for Gradio\n",
        "    history.append((user_input, assistant_reply))\n",
        "\n",
        "    return history, assistant_reply  # Return the updated history and assistant's response\n",
        "\n",
        "# Function for Article Summarization\n",
        "def interact_summarization(prompt: str, article: str, temp=1.0) -> str:\n",
        "    input_text = f\"{prompt}\\n{article}\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": input_text}],\n",
        "        temperature=temp,\n",
        "        max_tokens=200\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Reset function\n",
        "def reset():\n",
        "    return [], \"\"  # Return empty history and no response\n",
        "\n",
        "# Define Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Chatbot & Article Summarization\")\n",
        "\n",
        "    # ðŸ”¹ Chatbot Section\n",
        "    with gr.Tab(\"Chatbot\"):\n",
        "        chatbot = gr.Chatbot()\n",
        "        user_input = gr.Textbox(placeholder=\"Ask me anything...\")\n",
        "        send_button = gr.Button(\"Send\")\n",
        "        reset_button = gr.Button(\"Reset\")\n",
        "\n",
        "        send_button.click(chatbot_response, inputs=[user_input, chatbot], outputs=[chatbot, user_input])\n",
        "        reset_button.click(reset, outputs=[chatbot, user_input])\n",
        "\n",
        "    # ðŸ”¹ Summarization Section\n",
        "    with gr.Tab(\"Article Summarization\"):\n",
        "        gr.Markdown(\"## Provide an article to summarize\")\n",
        "\n",
        "        prompt_textbox = gr.Textbox(label=\"Prompt\", value=\"Please summarize the following article carefully:\", visible=False)\n",
        "        article_textbox = gr.Textbox(label=\"Article\", interactive=True, placeholder=\"Paste your article here...\")\n",
        "\n",
        "        temperature_slider = gr.Slider(0.0, 2.0, 1.0, step=0.1, label=\"Temperature\")\n",
        "\n",
        "        summarize_button = gr.Button(\"Summarize\")\n",
        "        summary_output = gr.Textbox(label=\"Summary\", interactive=False)\n",
        "\n",
        "        summarize_button.click(interact_summarization, inputs=[prompt_textbox, article_textbox, temperature_slider], outputs=[summary_output])\n",
        "\n",
        "# Launch Gradio Interface\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-3iZIeFOoYN"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}